{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://rsilveira79.github.io/fermenting_gradients/machine_learning/nlp/pytorch/pytorch-transformer-squad/\n",
    "#download model from https://drive.google.com/drive/folders/1OnvT5sKgi0WVWTXnTaaOPTE5KIh-xg_E\n",
    "#download model from https://drive.google.com/drive/folders/1e7wu9yI-rGkSzjoPU2TpCC9FMvlKvl8R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from pytorch_transformers import BertConfig, BertTokenizer, BertForQuestionAnswering\n",
    "from pytorch_transformers import XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForQuestionAnswering, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswering(object):\n",
    "    def __init__(self, config_file, weight_file, tokenizer_file, model_type ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.config_class, self.model_class, self.tokenizer_class = MODEL_CLASSES[model_type]\n",
    "        self.config = self.config_class.from_json_file(config_file)\n",
    "        self.model = self.model_class(self.config)\n",
    "        self.model.load_state_dict(torch.load(weight_file, map_location=self.device))\n",
    "        self.tokenizer = self.tokenizer_class(tokenizer_file)\n",
    "        self.model_type = model_type\n",
    "    \n",
    "    def to_list(self, tensor):\n",
    "        return tensor.detach().cpu().tolist()\n",
    "\n",
    "    def get_reply(self, question, passage):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_ids, _ , tokens = self.prepare_features(question, passage)\n",
    "            if self.model_type == 'bert':\n",
    "                span_start,span_end= self.model(input_ids)\n",
    "                answer = tokens[torch.argmax(span_start):torch.argmax(span_end)+1]\n",
    "                answer = self.bert_convert_tokens_to_string(answer)\n",
    "            elif self.model_type == 'xlnet':\n",
    "                input_vector = {'input_ids': input_ids,\n",
    "                                'start_positions': None,\n",
    "                                'end_positions': None }\n",
    "                outputs = self.model(**input_vector)\n",
    "                answer = tokens[self.to_list(outputs[1])[0][torch.argmax(outputs[0])]:self.to_list(outputs[3])[0][torch.argmax(outputs[2])]+1]\n",
    "                answer = self.xlnet_convert_tokens_to_string(answer)\n",
    "        return answer\n",
    "    \n",
    "    def bert_convert_tokens_to_string(self, tokens):\n",
    "        out_string = ' '.join(tokens).replace(' ##', '').strip()\n",
    "        if '@' in tokens:\n",
    "            out_string = out_string.replace(' ', '')\n",
    "        return out_string\n",
    "\n",
    "    def xlnet_convert_tokens_to_string(self, tokens):\n",
    "        out_string = ''.join(tokens).replace('▁', ' ').strip()\n",
    "        return out_string\n",
    "\n",
    "    def prepare_features(self, question,  passage, max_seq_length = 300, \n",
    "                 zero_pad = False, include_CLS_token = True, include_SEP_token = True):\n",
    "        ## Tokenzine Input\n",
    "        tokens_a = self.tokenizer.tokenize(question)\n",
    "        tokens_b = self.tokenizer.tokenize(passage)\n",
    "        ## Truncate\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "        ## Initialize Tokens\n",
    "        tokens = []\n",
    "        if include_CLS_token:\n",
    "            tokens.append(self.tokenizer.cls_token)\n",
    "        ## Add Tokens and separators\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "        if include_SEP_token:\n",
    "            tokens.append(self.tokenizer.sep_token)\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "        ## Convert Tokens to IDs\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        ## Input Mask \n",
    "        input_mask = [1] * len(input_ids)\n",
    "        ## Zero-pad sequence lenght\n",
    "        if zero_pad:\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "        return torch.tensor(input_ids).unsqueeze(0), input_mask, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = QuestionAnswering(\n",
    "    config_file =   'bert-large-cased-whole-word-masking-finetuned-squad-config.json',\n",
    "    weight_file=    'bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin',\n",
    "    tokenizer_file= 'bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt',\n",
    "    model_type =    'bert'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feed your Model With The Passage </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = '''Insect behavior generally appears to be explicable in terms of unconscious, inflexible stimulus-response\n",
    "mechanisms. For instance, a female sphex wasp leaves her egg sealed in a burrow alongside a paralyzed\n",
    "grasshopper, which her larvae can eat upon hatching. Before she deposits the grasshopper in the burrow, she\n",
    "inspects the burrow; if the inspection reveals no problems, she drags the grasshopper inside by its antennae. As\n",
    "thoughtful as this behavior appears, it reveals its mechanistic character upon interference. Darwin discovered\n",
    "that prior removal of the grasshopper's antennae prevents the wasp from depositing the grasshopper, even though\n",
    "the legs or ovipositor could also serve as handles. Likewise, Fabre moved the grasshopper a few centimeters\n",
    "away from the burrow's mouth while the wasp was inside inspecting. The wasp returned the grasshopper to the\n",
    "edge of the burrow and then began a new inspection. Fabre performed this disruptive maneuver forty times; the\n",
    "wasp's response never changed.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ask your Questoins in Here </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"The author mentions the work of Darwin and Fabre in order to?\",\n",
    "    \"Which of the following hypothetical variations in the experiments described in the passage would most weaken the primary claim of the passage?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Get your answers </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The author mentions the work of Darwin and Fabre in order to?\n",
      "Answer:   [CLS]\n",
      "Question: Which of the following hypothetical variations in the experiments described in the passage would most weaken the primary claim of the passage?\n",
      "Answer:   [CLS]\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    print('Question:',question)\n",
    "    print('Answer:  ',bert.get_reply(question,facts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
