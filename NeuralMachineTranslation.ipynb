{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is available on http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_examples_taken = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('deu.txt')\n",
    "text = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to Convert all text to lowercase, remove punctuations and separate German with english and encode special characters in german to utf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items(): \n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(trainY):\n",
    "    ylist = list()\n",
    "    for sequence in trainY:\n",
    "        encoded = to_categorical(sequence, num_classes=english_vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.asarray(ylist)\n",
    "    y = y.reshape(trainY.shape[0], trainY.shape[1], english_vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus_X = []\n",
    "exclude = set(string.punctuation)\n",
    "for i in range(len(text)):\n",
    "    temp_text = text[i].lower().replace('\\n','').split('\\t')\n",
    "    for j in range(len(temp_text)):\n",
    "        temp_text[j] = ''.join(ch for ch in temp_text[j] if ch not in exclude)\n",
    "        temp_text[j] = normalize('NFD', temp_text[j]).encode('ascii', 'ignore')\n",
    "        temp_text[j] = temp_text[j].decode('UTF-8') \n",
    "    final_corpus_X.append(temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences(sentence,model):\n",
    "    german_sentence = encode_sequences(german_tokenization, german_length, [sentence])\n",
    "    val = model.predict(german_sentence)\n",
    "    integers = [argmax(vector) for vector in val[0]]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, english_tokenization)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi => gru gott\n",
      "run => lauf\n",
      "wow => potzdonner\n",
      "wow => donnerwetter\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    print(final_corpus_X[i][0],'=>' ,final_corpus_X[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'hallo'],\n",
       "       ['hi', 'gru gott'],\n",
       "       ['run', 'lauf'],\n",
       "       ['wow', 'potzdonner'],\n",
       "       ['wow', 'donnerwetter']], dtype='<U370')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_corpus_X = np.asarray(final_corpus_X)\n",
    "final_corpus_X[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus_X = final_corpus_X[:total_examples_taken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x1354ebb00> 3808 6\n"
     ]
    }
   ],
   "source": [
    "english_tokenization = create_tokenizer(final_corpus_X[:,0])\n",
    "english_vocab_size = len(english_tokenization.word_index) + 1\n",
    "english_length = max_length(final_corpus_X[:, 0])\n",
    "print(english_tokenization, english_vocab_size, english_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x1390ef630> 5852 10\n"
     ]
    }
   ],
   "source": [
    "german_tokenization = create_tokenizer(final_corpus_X[:,1])\n",
    "german_vocab_size = len(german_tokenization.word_index) + 1\n",
    "german_length = max_length(final_corpus_X[:,1])\n",
    "print(german_tokenization, german_vocab_size, german_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = encode_sequences(german_tokenization, german_length, final_corpus_X[:, 1])\n",
    "trainY = encode_sequences(english_tokenization, english_length, final_corpus_X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = one_hot_encoding(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 6, 3808)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 39s - loss: 3.7255\n",
      "Epoch 2/30\n",
      " - 38s - loss: 3.0967\n",
      "Epoch 3/30\n",
      " - 35s - loss: 2.7955\n",
      "Epoch 4/30\n",
      " - 35s - loss: 2.5220\n",
      "Epoch 5/30\n",
      " - 36s - loss: 2.2922\n",
      "Epoch 6/30\n",
      " - 36s - loss: 2.0758\n",
      "Epoch 7/30\n",
      " - 36s - loss: 1.8731\n",
      "Epoch 8/30\n",
      " - 35s - loss: 1.6954\n",
      "Epoch 9/30\n",
      " - 35s - loss: 1.5349\n",
      "Epoch 10/30\n",
      " - 35s - loss: 1.3892\n",
      "Epoch 11/30\n",
      " - 48s - loss: 1.2572\n",
      "Epoch 12/30\n",
      " - 36s - loss: 1.1326\n",
      "Epoch 13/30\n",
      " - 35s - loss: 1.0189\n",
      "Epoch 14/30\n",
      " - 36s - loss: 0.9157\n",
      "Epoch 15/30\n",
      " - 35s - loss: 0.8209\n",
      "Epoch 16/30\n",
      " - 52s - loss: 0.7343\n",
      "Epoch 17/30\n",
      " - 44s - loss: 0.6582\n",
      "Epoch 18/30\n",
      " - 38s - loss: 0.5887\n",
      "Epoch 19/30\n",
      " - 35s - loss: 0.5278\n",
      "Epoch 20/30\n",
      " - 39s - loss: 0.4763\n",
      "Epoch 21/30\n",
      " - 35s - loss: 0.4293\n",
      "Epoch 22/30\n",
      " - 35s - loss: 0.3879\n",
      "Epoch 23/30\n",
      " - 35s - loss: 0.3524\n",
      "Epoch 24/30\n",
      " - 35s - loss: 0.3221\n",
      "Epoch 25/30\n",
      " - 35s - loss: 0.2949\n",
      "Epoch 26/30\n",
      " - 35s - loss: 0.2706\n",
      "Epoch 27/30\n",
      " - 35s - loss: 0.2508\n",
      "Epoch 28/30\n",
      " - 35s - loss: 0.2325\n",
      "Epoch 29/30\n",
      " - 35s - loss: 0.2153\n",
      "Epoch 30/30\n",
      " - 36s - loss: 0.2006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x135750f60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model(german_vocab_size, english_vocab_size, german_length, english_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(trainX, y, epochs=30, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'went', 'surfing']\n",
      "['i', 'couldnt', 'walk']\n",
      "['i', 'like', 'that']\n",
      "['i', 'love', 'you']\n",
      "['i', 'said', 'shut', 'it']\n",
      "['hows', 'your', 'dad']\n",
      "['i', 'always', 'walk']\n"
     ]
    }
   ],
   "source": [
    "sentences = ['er ging surfen','ich konnte nicht gehen','das gefallt mir','ich liebe dich','ich sagte du sollst den mund halten','wie geht es eurem vater','ich gehe immer zu fu']\n",
    "for i in sentences:\n",
    "    predict_sentences(i,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
